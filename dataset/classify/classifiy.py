#!/usr/bin/env python3
"""
Enhanced Domain Classification using Multiple SOTA Models
========================================================

This program provides maximum accuracy for domain classification by:
1. Using ensemble of 6 top embedding models (E5, MPNet, MiniLM, LaBSE, Jina, mDeBERTa)
2. Advanced preprocessing and feature engineering 
3. Domain-specific pattern recognition
4. Multi-level confidence scoring
5. Conflict resolution with retry logic (up to 3 attempts)
6. Vague classification detection (only assigns "Other" when truly ambiguous)

Features:
- Excludes "Other" category from initial classification
- Retries domains with tied predictions up to 3 times
- Marks persistent conflicts with asterisk (*)
- Only assigns "Other" for genuinely vague/ambiguous content
- Comprehensive statistics on conflicts and classification quality

Expected Input Structure:
- domain: Clean domain name (e.g., "wikipedia.org", not URLs)
- title: Page title
- meta_description: Meta description
- keywords: Keywords associated with the domain

Usage: python enhanced_domain_classifier.py <input_parquet_file>
"""

import pandas as pd
import numpy as np
import sys
import os
from tqdm import tqdm
import gc
import time
import warnings
import re
from urllib.parse import urlparse
from collections import Counter, defaultdict
import logging
import random

# ML Libraries
import torch
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib

# Clustering (if needed in future)
# from bertopic import BERTopic

# Suppress warnings
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedDomainClassifier:
    """Enhanced multi-model domain classifier with BERTopic"""
    
    def __init__(self, use_gpu=True):
        self.device = torch.device("cuda" if torch.cuda.is_available() and use_gpu else "cpu")
        self.models = {}
        self.category_embeddings = {}
        self.tld_patterns = {}
        self.domain_features = {}
        
        # Enhanced category definitions with Greek internal names and English output mapping
        self.categories = {
            "Î—Î»ÎµÎºÏ„ÏÎ¿Î½Î¹ÎºÏŒ Î•Î¼Ï€ÏŒÏÎ¹Î¿ & Î‘Î³Î¿ÏÎ­Ï‚": {
                "english_name": "E-Commerce & Shopping",
                "description": "Î·Î»ÎµÎºÏ„ÏÎ¿Î½Î¹ÎºÎ­Ï‚ Î±Î³Î¿ÏÎ­Ï‚, Î·Î»ÎµÎºÏ„ÏÎ¿Î½Î¹ÎºÏŒ ÎµÎ¼Ï€ÏŒÏÎ¹Î¿, Î»Î¹Î±Î½Î¹ÎºÏŒ ÎµÎ¼Ï€ÏŒÏÎ¹Î¿, Î±Î³Î¿ÏÎ¬, Î±Î³Î¿ÏÎ±Ï€Ï‰Î»Î·ÏƒÎ¯Î±, Ï€ÏŽÎ»Î·ÏƒÎ·, Ï€ÏÎ¿ÏŠÏŒÎ½Ï„Î±, ÎºÎ±Ï„Î±ÏƒÏ„Î®Î¼Î±Ï„Î±, Î¼Î±Î³Î±Î¶Î¹Î¬, ÎºÎ±Î»Î¬Î¸Î¹ Î±Î³Î¿ÏÏŽÎ½, Î¿Î»Î¿ÎºÎ»Î®ÏÏ‰ÏƒÎ· Ï€Î±ÏÎ±Î³Î³ÎµÎ»Î¯Î±Ï‚, Ï€Î»Î·ÏÏ‰Î¼Î®, Ï€Î±ÏÎ±Î³Î³ÎµÎ»Î¯ÎµÏ‚, Î·Î»ÎµÎºÏ„ÏÎ¿Î½Î¹ÎºÏŒ ÎºÎ±Ï„Î¬ÏƒÏ„Î·Î¼Î±, eshop, Î±Î³Î¿ÏÎ­Ï‚, ÎºÎ±Ï„Î¬ÏƒÏ„Î·Î¼Î±, Ï€Ï‰Î»Î®ÏƒÎµÎ¹Ï‚, Ï€ÏÎ¿ÏŠÏŒÎ½Ï„Î±, ÎµÎ¼Ï€ÏŒÏÎ¹Î¿, Î±Î³Î¿ÏÎ¬, Ï€Î»Î·ÏÏ‰Î¼Î®, Ï€Î±ÏÎ±Î³Î³ÎµÎ»Î¯Î±, ÎºÎ±Î»Î¬Î¸Î¹, Î­Î¼Ï€Î¿ÏÎ¿Ï‚, Ï€ÏÎ¿Î¼Î·Î¸ÎµÏ…Ï„Î®Ï‚, ÎµÎ¼Ï€ÏŒÏÎ¹Î¿, ÏƒÏ…Î½Î±Î»Î»Î±Î³Î­Ï‚, ÎµÏ€Î¹Ï‡ÎµÎ¹ÏÎ®ÏƒÎµÎ¹Ï‚, Ï€Ï‰Î»Î®ÏƒÎµÎ¹Ï‚, Ï€ÎµÎ»Î¬Ï„Î·Ï‚, Î±Î³Î¿ÏÎ¬, ÏƒÏ…Î½Î±Î»Î»Î±Î³Î®, online shopping, retail, marketplace, merchant, vendor, checkout, payment, cart, order, purchase, sale, discount, deal, offer, coupon, price, product, item, inventory, catalog, store, shop, business, commercial, trade, selling, buying",
                "keywords": ["Î±Î³Î¿ÏÎ­Ï‚", "ÎºÎ±Ï„Î¬ÏƒÏ„Î·Î¼Î±", "Ï€Ï‰Î»Î®ÏƒÎµÎ¹Ï‚", "Ï€ÏÎ¿ÏŠÏŒÎ½Ï„Î±", "ÎµÎ¼Ï€ÏŒÏÎ¹Î¿", "Î±Î³Î¿ÏÎ¬", "Ï€Î»Î·ÏÏ‰Î¼Î®", "Ï€Î±ÏÎ±Î³Î³ÎµÎ»Î¯Î±", "ÎºÎ±Î»Î¬Î¸Î¹", "eshop", "Î·Î»ÎµÎºÏ„ÏÎ¿Î½Î¹ÎºÏŒ", "Î¼Î±Î³Î±Î¶Î¯", "Î±Î³Î¿ÏÎ±Ï€Ï‰Î»Î·ÏƒÎ¯Î±", "ÏƒÏ…Î½Î±Î»Î»Î±Î³Î®", "Ï€ÎµÎ»Î¬Ï„Î·Ï‚", "Î­Î¼Ï€Î¿ÏÎ¿Ï‚", "Ï€ÏÎ¿Î¼Î·Î¸ÎµÏ…Ï„Î®Ï‚", "Ï„Î¹Î¼Î®", "Ï€ÏÎ¿ÏƒÏ†Î¿ÏÎ¬", "ÎµÎºÏ€Ï„ÏŽÏƒÎµÎ¹Ï‚", "Î»Î¹Î±Î½Î¹ÎºÏŒ", "Ï‡Î¿Î½Î´ÏÎ¹ÎºÏŒ", "marketplace", "Ï€Î»Î±Ï„Ï†ÏŒÏÎ¼Î±", "shop", "store", "buy", "sell", "ecommerce", "shopping", "retail", "marketplace", "merchant", "vendor", "checkout", "payment", "cart", "order", "purchase", "sale", "discount", "deal", "offer", "coupon", "price", "product", "item", "inventory", "catalog", "business", "commercial", "trade", "selling", "buying", "online", "market", "bazaar", "outlet", "boutique", "supermarket", "mall", "department", "wholesale", "dropship", "fulfillment", "logistics", "shipping", "delivery"],
                "tlds": [".shop", ".store", ".buy", ".sale", ".market", ".deals"],
                "domains": ["amazon", "ebay", "alibaba", "shopify", "etsy", "walmart", "target", "skroutz", "bestprice", "plaisio", "aliexpress", "wish", "overstock", "newegg", "bestbuy", "costco", "homedepot", "lowes", "macys", "nordstrom", "zappos", "wayfair", "ikea", "zara", "hm", "uniqlo", "nike", "adidas", "sephora", "ulta", "cvs", "walgreens", "pharmacy", "grocery", "food", "restaurant", "delivery", "uber", "lyft", "doordash", "grubhub", "instacart", "postmates"]
            },
            "Î•Î¹Î´Î®ÏƒÎµÎ¹Ï‚ & ÎœÎ­ÏƒÎ± Î•Î½Î·Î¼Î­ÏÏ‰ÏƒÎ·Ï‚": {
                "english_name": "News & Media",
                "description": "ÎµÎ¹Î´Î®ÏƒÎµÎ¹Ï‚, Î´Î·Î¼Î¿ÏƒÎ¹Î¿Î³ÏÎ±Ï†Î¯Î±, ÎµÏ†Î·Î¼ÎµÏÎ¯Î´ÎµÏ‚, Ï€ÎµÏÎ¹Î¿Î´Î¹ÎºÎ¬, Î¼Î­ÏƒÎ± ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ·Ï‚, Ï„ÏÏ€Î¿Ï‚, ÏÎ±Î´Î¹Î¿Ï„Î·Î»ÎµÏŒÏÎ±ÏƒÎ·, Ï„Î·Î»ÎµÏŒÏÎ±ÏƒÎ·, ÏÎ±Î´Î¹ÏŒÏ†Ï‰Î½Î¿, Î¬ÏÎ¸ÏÎ±, Î´Î·Î¼Î¿ÏƒÎ¹Î¿Î³ÏÎ¬Ï†Î¿Î¹, Î­ÎºÏ„Î±ÎºÏ„Î± Î½Î­Î±, Ï„Î¯Ï„Î»Î¿Î¹, Ï„ÏÎ­Ï‡Î¿Î½Ï„Î± Î³ÎµÎ³Î¿Î½ÏŒÏ„Î±, Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ®, Ï€Î±Î³ÎºÏŒÏƒÎ¼Î¹Î± Î½Î­Î±, Ï„Î¿Ï€Î¹ÎºÎ¬ Î½Î­Î±, ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ·, ÏÎµÏ€Î¿ÏÏ„Î¬Î¶, ÏƒÏ…Î½Î­Î½Ï„ÎµÏ…Î¾Î·, Î±Î½Î±Î»ÏÏƒÎµÎ¹Ï‚, ÏƒÏ‡Î¿Î»Î¹Î±ÏƒÎ¼ÏŒÏ‚, Î¼ÎµÏ„Î¬Î´Î¿ÏƒÎ·, ÎµÎºÏ€Î¿Î¼Ï€Î­Ï‚, Î´ÎµÎ»Ï„Î¯Î± ÎµÎ¹Î´Î®ÏƒÎµÏ‰Î½, ÎºÎ±Î½Î¬Î»Î¹Î±, Î´Î·Î¼Î¿ÏƒÎ¹Î¿Î³ÏÎ±Ï†Î¹ÎºÏŒ Î­ÏÎ³Î¿, ÏÎµÏ€ÏŒÏÏ„ÎµÏ, breaking news, current events, headlines, journalism, reporter, correspondent, editorial, opinion, commentary, broadcast, television, radio, magazine, newspaper, press release, media outlet, news agency, newsroom, anchor, journalist, editor, publisher",
                "keywords": ["ÎµÎ¹Î´Î®ÏƒÎµÎ¹Ï‚", "ÎµÏ†Î·Î¼ÎµÏÎ¯Î´Î±", "Î½Î­Î±", "Î´Î·Î¼Î¿ÏƒÎ¹Î¿Î³ÏÎ±Ï†Î¯Î±", "ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎ·", "Ï„Î·Î»ÎµÏŒÏÎ±ÏƒÎ·", "ÏÎ±Î´Î¹ÏŒÏ†Ï‰Î½Î¿", "Î¬ÏÎ¸ÏÎ±", "Î´Î·Î¼Î¿ÏƒÎ¹Î¿Î³ÏÎ¬Ï†Î¿Ï‚", "Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ®", "Ï„Î¯Ï„Î»Î¿Î¹", "ÏÎµÏ€Î¿ÏÏ„Î¬Î¶", "Î­ÎºÏ„Î±ÎºÏ„Î±", "ÎºÎ±Î½Î¬Î»Î¹", "ÎµÎºÏ€Î¿Î¼Ï€Î®", "Î´ÎµÎ»Ï„Î¯Î¿", "ÏƒÏ‡Î¿Î»Î¹Î±ÏƒÎ¼ÏŒÏ‚", "Î±Î½Î¬Î»Ï…ÏƒÎ·", "ÏƒÏ…Î½Î­Î½Ï„ÎµÏ…Î¾Î·", "Î¼ÎµÏ„Î¬Î´Î¿ÏƒÎ·", "Ï€ÎµÏÎ¹Î¿Î´Î¹ÎºÏŒ", "Ï„ÏÏ€Î¿Ï‚", "ÏÎµÏ€ÏŒÏÏ„ÎµÏ", "Î±ÏÎ¸ÏÎ¿Î³ÏÎ¬Ï†Î¿Ï‚", "ÏƒÏ…Î½Ï„Î¬ÎºÏ„Î·Ï‚", "ÎµÎºÎ´ÏŒÏ„Î·Ï‚", "news", "media", "press", "journalism", "newspaper", "magazine", "reporter", "journalist", "breaking", "headline", "story", "article", "editorial", "opinion", "commentary", "broadcast", "television", "radio", "correspondent", "anchor", "editor", "publisher", "newsroom", "agency", "outlet", "current", "events", "politics", "world", "local", "national", "international", "daily", "weekly", "monthly"],
                "tlds": [".news", ".press", ".media"],
                "domains": ["cnn", "bbc", "reuters", "nytimes", "guardian", "kathimerini", "iefimerida", "newsbeast", "news247", "cnn.gr", "protothema", "tovima", "naftemporiki", "skai", "ap", "afp", "bloomberg", "wsj", "ft", "economist", "time", "newsweek", "usatoday", "washingtonpost", "latimes", "telegraph", "independent", "mirror", "sun", "dailymail", "foxnews", "msnbc", "abc", "cbs", "nbc", "espn", "sky", "itv", "channel4"]
            },
            "ÎšÎ¿Î¹Î½Ï‰Î½Î¹ÎºÎ¬ Î”Î¯ÎºÏ„Ï…Î± & ÎšÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±": {
                "english_name": "Social Media & Community",
                "description": "ÎºÎ¿Î¹Î½Ï‰Î½Î¹ÎºÎ¬ Î´Î¯ÎºÏ„Ï…Î±, ÎºÎ¿Î¹Î½Ï‰Î½Î¹ÎºÎ¬ Î¼Î­ÏƒÎ±, ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±, Ï†ÏŒÏÎ¿Ï…Î¼, ÏƒÏ…Î¶Î®Ï„Î·ÏƒÎ·, ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±, Î¼Î·Î½ÏÎ¼Î±Ï„Î±, Î´Î¹ÎºÏ„ÏÏ‰ÏƒÎ·, ÎºÎ¿Î¹Î½Ï‰Î½Î¹ÎºÎ® Î±Î»Î»Î·Î»ÎµÏ€Î¯Î´ÏÎ±ÏƒÎ·, Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿ Ï‡ÏÎ·ÏƒÏ„ÏŽÎ½, ÎºÎ¿Î¹Î½Î¿Ï€Î¿Î¯Î·ÏƒÎ·, Î±Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚, ÏƒÏ‡ÏŒÎ»Î¹Î±, likes, Î±ÎºÏŒÎ»Î¿Ï…Î¸Î¿Î¹, Ï†Î¯Î»Î¿Î¹, ÏƒÏ…Î½Î´Î­ÏƒÎµÎ¹Ï‚, Î¿Î¼Î¬Î´ÎµÏ‚, ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„ÎµÏ‚, ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±, Î´Î¹Î±Î¼Î¿Î¹ÏÎ±ÏƒÎ¼ÏŒÏ‚, ÏƒÏ…Î¶Î·Ï„Î®ÏƒÎµÎ¹Ï‚, Î´Î¹Î±Î´ÏÎ±ÏƒÏ„Î¹ÎºÏŒÏ„Î·Ï„Î±, Ï‡ÏÎ®ÏƒÏ„ÎµÏ‚, Ï€ÏÎ¿Ï†Î¯Î», Î´Î·Î¼Î¿ÏƒÎ¹ÎµÏÏƒÎµÎ¹Ï‚, Î±Î»Î»Î·Î»ÎµÏ€Î¯Î´ÏÎ±ÏƒÎ·, Î´Î¯ÎºÏ„Ï…Î¿ ÎµÏ€Î±Ï†ÏŽÎ½",
                "keywords": ["ÎºÎ¿Î¹Î½Ï‰Î½Î¹ÎºÎ¬", "ÎºÎ¿Î¹Î½ÏŒÏ„Î·Ï„Î±", "Ï†ÏŒÏÎ¿Ï…Î¼", "ÏƒÏ…Î¶Î®Ï„Î·ÏƒÎ·", "Î¼Î·Î½ÏÎ¼Î±Ï„Î±", "Î´Î¹ÎºÏ„ÏÏ‰ÏƒÎ·", "ÎºÎ¿Î¹Î½Î¿Ï€Î¿Î¯Î·ÏƒÎ·", "Î±Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚", "ÏƒÏ‡ÏŒÎ»Î¹Î±", "Ï†Î¯Î»Î¿Î¹", "Î±ÎºÏŒÎ»Î¿Ï…Î¸Î¿Î¹", "Î¿Î¼Î¬Î´ÎµÏ‚", "ÎµÏ€Î¹ÎºÎ¿Î¹Î½Ï‰Î½Î¯Î±", "Î´Î¹Î±Î¼Î¿Î¹ÏÎ±ÏƒÎ¼ÏŒÏ‚", "ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±", "Ï€ÏÎ¿Ï†Î¯Î»", "Î´Î·Î¼Î¿ÏƒÎ¹ÎµÏÏƒÎµÎ¹Ï‚", "Î±Î»Î»Î·Î»ÎµÏ€Î¯Î´ÏÎ±ÏƒÎ·", "Ï‡ÏÎ®ÏƒÏ„ÎµÏ‚", "Î´Î¯ÎºÏ„Ï…Î¿", "social", "community", "forum", "chat"],
                "tlds": [".social", ".community", ".forum"],
                "domains": ["facebook", "twitter", "instagram", "linkedin", "reddit", "discord", "telegram", "whatsapp", "tiktok", "snapchat", "youtube"]
            },
            "Î¤ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î± & Î›Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ": {
                "english_name": "Technology & Software",
                "description": "Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±, Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ, Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚, Î±Î½Î¬Ï€Ï„Ï…Î¾Î·, Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹ÎºÎ®, Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î­Ï‚, ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚, ÏˆÎ·Ï†Î¹Î±ÎºÎ¬ ÎµÏÎ³Î±Î»ÎµÎ¯Î±, Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¹ÎºÎ­Ï‚ ÎµÏ„Î±Î¹ÏÎµÎ¯ÎµÏ‚, ÎºÏŽÎ´Î¹ÎºÎ±Ï‚, Î±Î½Î¬Ï€Ï„Ï…Î¾Î· Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÎ¿Ï, Ï…Î»Î¹ÎºÏŒ, Î·Î»ÎµÎºÏ„ÏÎ¿Î½Î¹ÎºÎ¬, ÎºÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¯Î±, Î½ÎµÎ¿Ï†Ï…Î®Ï‚ ÎµÏ€Î¹Ï‡ÎµÎ¯ÏÎ·ÏƒÎ·, Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¹ÎºÎ¬ Î½Î­Î±, Ï„ÎµÏ‡Î½Î·Ï„Î® Î½Î¿Î·Î¼Î¿ÏƒÏÎ½Î·, Î¼Î·Ï‡Î±Î½Î¹ÎºÎ® Î¼Î¬Î¸Î·ÏƒÎ·, Î´ÎµÎ´Î¿Î¼Î­Î½Î±, cloud, Ï€Î»Î±Ï„Ï†ÏŒÏÎ¼Î±, Î´Î¹Î±Î´Î¯ÎºÏ„Ï…Î¿, Î¹ÏƒÏ„Î¿ÏƒÎµÎ»Î¯Î´ÎµÏ‚, ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î±, Î²Î¬ÏƒÎµÎ¹Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½, Î´Î¯ÎºÏ„Ï…Î±, Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±, ÎºÏ…Î²ÎµÏÎ½Î¿Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±, mobile apps, web development, APIs, frameworks, libraries, databases, servers, hosting, SaaS, DevOps, automation, artificial intelligence, machine learning, data science, cybersecurity, blockchain, cryptocurrency, fintech, startup, innovation, digital transformation",
                "keywords": ["Ï„ÎµÏ‡Î½Î¿Î»Î¿Î³Î¯Î±", "Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ", "Ï€ÏÎ¿Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÏƒÎ¼ÏŒÏ‚", "Î±Î½Î¬Ï€Ï„Ï…Î¾Î·", "Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÏ„Î­Ï‚", "ÎµÏ†Î±ÏÎ¼Î¿Î³Î­Ï‚", "ÎºÏŽÎ´Î¹ÎºÎ±Ï‚", "Î·Î»ÎµÎºÏ„ÏÎ¿Î½Î¹ÎºÎ¬", "ÎºÎ±Î¹Î½Î¿Ï„Î¿Î¼Î¯Î±", "Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹ÎºÎ®", "ÏˆÎ·Ï†Î¹Î±ÎºÎ¬", "Ï„ÎµÏ‡Î½Î·Ï„Î®", "Î½Î¿Î·Î¼Î¿ÏƒÏÎ½Î·", "Î´ÎµÎ´Î¿Î¼Î­Î½Î±", "cloud", "Ï€Î»Î±Ï„Ï†ÏŒÏÎ¼Î±", "Î´Î¹Î±Î´Î¯ÎºÏ„Ï…Î¿", "Î¹ÏƒÏ„Î¿ÏƒÎµÎ»Î¯Î´ÎµÏ‚", "ÏƒÏ…ÏƒÏ„Î®Î¼Î±Ï„Î±", "Î²Î¬ÏƒÎµÎ¹Ï‚", "Î´Î¯ÎºÏ„Ï…Î±", "Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±", "ÎºÏ…Î²ÎµÏÎ½Î¿Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±", "ÎµÏ†Î±ÏÎ¼Î¿Î³Î®", "Ï€Î»Î±Ï„Ï†ÏŒÏÎ¼ÎµÏ‚", "servers", "hosting", "tech", "software", "programming", "development", "coding", "developer", "engineer", "computer", "digital", "technology", "innovation", "startup", "app", "application", "platform", "framework", "library", "database", "server", "cloud", "api", "saas", "devops", "automation", "artificial", "intelligence", "machine", "learning", "data", "science", "cybersecurity", "security", "blockchain", "cryptocurrency", "fintech", "mobile", "web", "internet", "online", "digital", "electronic", "hardware", "software", "system", "network", "infrastructure", "solution", "service", "tool", "product"],
                "tlds": [".tech", ".app", ".dev", ".ai", ".io", ".software", ".cloud", ".digital"],
                "domains": ["github", "stackoverflow", "microsoft", "google", "apple", "amazon", "facebook", "netflix", "adobe", "oracle", "ibm", "intel", "cisco", "salesforce", "slack", "zoom", "dropbox", "atlassian", "jetbrains", "gitlab", "bitbucket", "heroku", "aws", "azure", "gcp", "digitalocean", "linode", "vultr", "cloudflare", "fastly", "stripe", "paypal", "twilio", "sendgrid", "mailgun", "firebase", "mongodb", "redis", "postgresql", "mysql", "docker", "kubernetes", "jenkins", "travis", "circleci", "npm", "yarn", "composer", "pip", "maven", "gradle"]
            },
            "Î¨Ï…Ï‡Î±Î³Ï‰Î³Î¯Î± & ÎœÎ­ÏƒÎ±": {
                "english_name": "Entertainment & Media",
                "description": "ÏˆÏ…Ï‡Î±Î³Ï‰Î³Î¯Î±, Ï„Î±Î¹Î½Î¯ÎµÏ‚, Î¼Î¿Ï…ÏƒÎ¹ÎºÎ®, Ï€Î±Î¹Ï‡Î½Î¯Î´Î¹Î±, gaming, Î´Î¹Î±ÏƒÎ·Î¼ÏŒÏ„Î·Ï„ÎµÏ‚, Ï„Î­Ï‡Î½ÎµÏ‚, Ï€Î¿Î»Î¹Ï„Î¹ÏƒÎ¼ÏŒÏ‚, Î´Î¹Î±ÏƒÎºÎ­Î´Î±ÏƒÎ·, ÎµÎ»ÎµÏÎ¸ÎµÏÎ¿Ï‚ Ï‡ÏÏŒÎ½Î¿Ï‚, ÎµÎºÏ€Î¿Î¼Ï€Î­Ï‚, ÏƒÏ…Î½Î±Ï…Î»Î¯ÎµÏ‚, Ï†ÎµÏƒÏ„Î¹Î²Î¬Î», streaming, Î²Î¯Î½Ï„ÎµÎ¿, Î®Ï‡Î¿Ï‚, Ï€Î¿Î»Ï…Î¼Î­ÏƒÎ±, Î´Î·Î¼Î¹Î¿Ï…ÏÎ³Î¹ÎºÏŒ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿, ÎºÎ¹Î½Î·Î¼Î±Ï„Î¿Î³ÏÎ¬Ï†Î¿Ï‚, Î¸Î­Î±Ï„ÏÎ¿, Ï„Î·Î»ÎµÎ¿Ï€Ï„Î¹ÎºÎ­Ï‚ ÏƒÎµÎ¹ÏÎ­Ï‚, Î½Ï„Î¿ÎºÎ¹Î¼Î±Î½Ï„Î­Ï, podcast, ÏÎ±Î´Î¹Î¿Ï†Ï‰Î½Î¹ÎºÎ­Ï‚ ÎµÎºÏ€Î¿Î¼Ï€Î­Ï‚, ÎºÎ±Î»Î»Î¹Ï„Î­Ï‡Î½ÎµÏ‚, Î¼Î¿Ï…ÏƒÎ¹ÎºÎ¿Î¯, Î·Î¸Î¿Ï€Î¿Î¹Î¿Î¯, ÏƒÎºÎ·Î½Î¿Î¸Î­Ï„ÎµÏ‚, Ï€Î±ÏÎ±Î³Ï‰Î³Î¿Î¯, Î´Î¹Î±ÏƒÎºÎ­Î´Î±ÏƒÎ·, hobby, Ï‡ÏŒÎ¼Ï€Î¹, video games, movies, films, cinema, television, TV shows, series, music, songs, albums, artists, bands, concerts, festivals, comedy, humor, celebrities, gossip, entertainment news, pop culture, lifestyle, fashion, beauty, art, culture, creative, content, media, streaming, video, audio, multimedia, digital content, online entertainment",
                "keywords": ["ÏˆÏ…Ï‡Î±Î³Ï‰Î³Î¯Î±", "Ï„Î±Î¹Î½Î¯ÎµÏ‚", "Î¼Î¿Ï…ÏƒÎ¹ÎºÎ®", "Ï€Î±Î¹Ï‡Î½Î¯Î´Î¹Î±", "Î´Î¹Î±ÏƒÎ·Î¼ÏŒÏ„Î·Ï„ÎµÏ‚", "Ï„Î­Ï‡Î½ÎµÏ‚", "Ï€Î¿Î»Î¹Ï„Î¹ÏƒÎ¼ÏŒÏ‚", "Î´Î¹Î±ÏƒÎºÎ­Î´Î±ÏƒÎ·", "ÎµÎºÏ€Î¿Î¼Ï€Î­Ï‚", "ÏƒÏ…Î½Î±Ï…Î»Î¯ÎµÏ‚", "Ï†ÎµÏƒÏ„Î¹Î²Î¬Î»", "streaming", "Î²Î¯Î½Ï„ÎµÎ¿", "Î®Ï‡Î¿Ï‚", "ÎºÎ¹Î½Î·Î¼Î±Ï„Î¿Î³ÏÎ¬Ï†Î¿Ï‚", "Î¸Î­Î±Ï„ÏÎ¿", "ÏƒÎµÎ¹ÏÎ­Ï‚", "Î½Ï„Î¿ÎºÎ¹Î¼Î±Î½Ï„Î­Ï", "podcast", "ÎºÎ±Î»Î»Î¹Ï„Î­Ï‡Î½ÎµÏ‚", "Î¼Î¿Ï…ÏƒÎ¹ÎºÎ¿Î¯", "Î·Î¸Î¿Ï€Î¿Î¹Î¿Î¯", "Ï‡ÏŒÎ¼Ï€Î¹", "gaming", "Ï€Î±Î¹Ï‡Î½Î¯Î´Î¹Î±", "ÎºÏ‰Î¼Ï‰Î´Î¯Î±", "Ï‡Î¹Î¿ÏÎ¼Î¿Ï", "lifestyle", "Î¼ÏŒÎ´Î±", "Î¿Î¼Î¿ÏÏ†Î¹Î¬", "entertainment", "music", "movies", "games", "gaming", "films", "cinema", "television", "tv", "shows", "series", "songs", "albums", "artists", "bands", "concerts", "festivals", "comedy", "humor", "celebrities", "gossip", "pop", "culture", "lifestyle", "fashion", "beauty", "art", "creative", "content", "media", "streaming", "video", "audio", "multimedia", "digital", "online", "fun", "leisure", "hobby", "recreation", "amusement", "enjoyment", "pleasure", "relaxation"],
                "tlds": [".entertainment", ".music", ".game", ".tv", ".video", ".media", ".fun"],
                "domains": ["netflix", "youtube", "spotify", "twitch", "steam", "imdb", "hulu", "disney", "warner", "sony", "universal", "paramount", "hbo", "showtime", "starz", "amazon", "prime", "apple", "music", "pandora", "soundcloud", "vimeo", "dailymotion", "metacritic", "rottentomatoes", "fandango", "ticketmaster", "stubhub", "eventbrite", "buzzfeed", "tmz", "eonline", "people", "usmagazine", "rollingstone", "billboard", "variety", "hollywoodreporter", "deadline", "ign", "gamespot", "polygon", "kotaku", "destructoid", "pcgamer", "gameinformer"]
            },
            "Î•ÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ· & ÎˆÏÎµÏ…Î½Î±": {
                "english_name": "Education & Research",
                "description": "ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·, ÏƒÏ‡Î¿Î»ÎµÎ¯Î±, Ï€Î±Î½ÎµÏ€Î¹ÏƒÏ„Î®Î¼Î¹Î±, Î¼Î¬Î¸Î·ÏƒÎ·, Î±ÎºÎ±Î´Î·Î¼Î±ÏŠÎºÎ¬, Î­ÏÎµÏ…Î½Î±, ÎµÏ€Î¹ÏƒÏ„Î®Î¼Î·, ÏƒÏ€Î¿Ï…Î´Î­Ï‚, Î¼Î±Î¸Î®Î¼Î±Ï„Î±, ÎºÎ±Ï„Î¬ÏÏ„Î¹ÏƒÎ·, Î³Î½ÏŽÏƒÎ·, ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ¿Î¯ Ï€ÏŒÏÎ¿Î¹, Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎµÏ‚, Î±ÎºÎ±Î´Î·Î¼Î±ÏŠÎºÎ¬ Î¹Î´ÏÏÎ¼Î±Ï„Î±, ÎµÏ€Î¹ÏƒÏ„Î·Î¼Î¿Î½Î¹ÎºÏŒ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿, Î´Î¹Î´Î±ÏƒÎºÎ±Î»Î¯Î±, Î¼Î¬Î¸Î·ÏƒÎ·, Ï†Î¿Î¹Ï„Î·Ï„Î­Ï‚, ÎºÎ±Î¸Î·Î³Î·Ï„Î­Ï‚, ÎµÏÎµÏ…Î½Î·Ï„Î­Ï‚, ÏƒÏ€Î¿Ï…Î´Î±ÏƒÏ„Î­Ï‚, ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÎ¬ Ï€ÏÎ¿Î³ÏÎ¬Î¼Î¼Î±Ï„Î±, ÏƒÎµÎ¼Î¹Î½Î¬ÏÎ¹Î±, Î´Î¹Î±Î»Î­Î¾ÎµÎ¹Ï‚, ÎµÏÎ³Î±ÏƒÏ„Î®ÏÎ¹Î±, Ï€Ï„Ï…Ï‡Î¯Î±, Î¼ÎµÏ„Î±Ï€Ï„Ï…Ï‡Î¹Î±ÎºÎ¬, Î´Î¹Î´Î±ÎºÏ„Î¿ÏÎ¹ÎºÎ¬, ÎµÏ€Î¹ÏƒÏ„Î·Î¼Î¿Î½Î¹ÎºÎ­Ï‚ Î´Î·Î¼Î¿ÏƒÎ¹ÎµÏÏƒÎµÎ¹Ï‚, ÎµÎ³ÎºÏ…ÎºÎ»Î¿Ï€Î±Î¯Î´ÎµÎ¹Î±, Î»ÎµÎ¾Î¹ÎºÏŒ, Î±Î½Î±Ï†Î¿ÏÎ¬, Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚, Î³Î½ÏŽÏƒÎ·, Î¼Î¬Î¸Î·ÏƒÎ·, ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÏŒ Ï…Î»Î¹ÎºÏŒ, Î±ÎºÎ±Î´Î·Î¼Î±ÏŠÎºÎ­Ï‚ Ï€Î·Î³Î­Ï‚, ÎµÏÎµÏ…Î½Î·Ï„Î¹ÎºÎ¬ ÎºÎ­Î½Ï„ÏÎ±, ÎµÏ€Î¹ÏƒÏ„Î·Î¼Î¿Î½Î¹ÎºÎ¬ Ï€ÎµÏÎ¹Î¿Î´Î¹ÎºÎ¬, Î´Î¹Î´Î±ÎºÏ„Î¹ÎºÏŒ Ï…Î»Î¹ÎºÏŒ, online courses, tutorials, reference, documentation",
                "keywords": ["ÎµÎºÏ€Î±Î¯Î´ÎµÏ…ÏƒÎ·", "ÏƒÏ‡Î¿Î»ÎµÎ¯Î¿", "Ï€Î±Î½ÎµÏ€Î¹ÏƒÏ„Î®Î¼Î¹Î¿", "Î¼Î¬Î¸Î·ÏƒÎ·", "Î±ÎºÎ±Î´Î·Î¼Î±ÏŠÎºÎ¬", "Î­ÏÎµÏ…Î½Î±", "ÎµÏ€Î¹ÏƒÏ„Î®Î¼Î·", "ÏƒÏ€Î¿Ï…Î´Î­Ï‚", "Î¼Î±Î¸Î®Î¼Î±Ï„Î±", "Î³Î½ÏŽÏƒÎ·", "Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ·", "ÎºÎ±Î¸Î·Î³Î·Ï„Î®Ï‚", "Ï†Î¿Î¹Ï„Î·Ï„Î®Ï‚", "Î´Î¹Î´Î±ÏƒÎºÎ±Î»Î¯Î±", "ÎºÎ±Ï„Î¬ÏÏ„Î¹ÏƒÎ·", "ÏƒÎµÎ¼Î¹Î½Î¬ÏÎ¹Î±", "Î´Î¹Î±Î»Î­Î¾ÎµÎ¹Ï‚", "ÎµÏÎ³Î±ÏƒÏ„Î®ÏÎ¹Î±", "Ï€Ï„Ï…Ï‡Î¯Î±", "Î¼ÎµÏ„Î±Ï€Ï„Ï…Ï‡Î¹Î±ÎºÎ¬", "Î´Î¹Î´Î±ÎºÏ„Î¿ÏÎ¹ÎºÎ¬", "ÎµÏÎµÏ…Î½Î·Ï„Î­Ï‚", "ÎµÎ³ÎºÏ…ÎºÎ»Î¿Ï€Î±Î¯Î´ÎµÎ¹Î±", "Î»ÎµÎ¾Î¹ÎºÏŒ", "Î±Î½Î±Ï†Î¿ÏÎ¬", "Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚", "Î±ÎºÎ±Î´Î·Î¼Î±ÏŠÎºÎ­Ï‚", "ÎµÏÎµÏ…Î½Î·Ï„Î¹ÎºÎ¬", "ÎµÏ€Î¹ÏƒÏ„Î·Î¼Î¿Î½Î¹ÎºÎ¬", "Î´Î¹Î´Î±ÎºÏ„Î¹ÎºÏŒ", "ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Ï„Î¹ÎºÏŒ", "education", "university", "research", "science", "wikipedia", "wiki", "encyclopedia", "dictionary", "reference", "academic", "scholar", "learning", "knowledge", "tutorial", "course", "study", "library", "documentation", "manual", "guide", "textbook", "journal", "publication", "thesis", "dissertation", "paper", "article", "scholarly"],
                "tlds": [".edu", ".ac", ".university", ".school", ".academy"],
                "domains": ["mit", "harvard", "stanford", "cambridge", "oxford", "coursera", "edx", "khan", "udemy", "skillshare", "uoa", "ntua", "auth", "upatras", "wikipedia", "wiki", "wikimedia", "wiktionary", "britannica", "encyclopedia", "dictionary", "reference", "scholar", "researchgate", "academia", "arxiv", "pubmed", "jstor", "springer", "elsevier", "nature", "science", "ieee", "acm"]
            },
            "Î¥Î³ÎµÎ¯Î± & Î™Î±Ï„ÏÎ¹ÎºÎ®": {
                "english_name": "Health & Medical",
                "description": "Ï…Î³ÎµÎ¯Î±, Î¹Î±Ï„ÏÎ¹ÎºÎ®, Ï…Î³ÎµÎ¹Î¿Î½Î¿Î¼Î¹ÎºÎ® Ï€ÎµÏÎ¯Î¸Î±Î»ÏˆÎ·, Î½Î¿ÏƒÎ¿ÎºÎ¿Î¼ÎµÎ¯Î±, Î³Î¹Î±Ï„ÏÎ¿Î¯, Ï†Î¬ÏÎ¼Î±ÎºÎ±, ÎµÏ…ÎµÎ¾Î¯Î±, Ï†Ï…ÏƒÎ¹ÎºÎ® ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·, Ï†Î±ÏÎ¼Î±ÎºÎµÎ¯Î¿, Î¹Î±Ï„ÏÎ¹ÎºÎ­Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚, Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Ï…Î³ÎµÎ¯Î±Ï‚, Î±ÏƒÎ¸Î­Î½ÎµÎ¹Î±, Î¸ÎµÏÎ±Ï€ÎµÎ¯Î±, Î´Î¹Î¬Î³Î½Ï‰ÏƒÎ·, Î¹Î±Ï„ÏÎ¹ÎºÎ® Î­ÏÎµÏ…Î½Î±, ÎºÎ»Î¹Î½Î¹ÎºÎ­Ï‚, Î¹Î±Ï„ÏÎ¿Î¯, Î½Î¿ÏƒÎ·Î»ÎµÏ…Ï„Î­Ï‚, Ï†Î±ÏÎ¼Î±ÎºÎµÏ…Ï„Î¹ÎºÎ¬, Î¹Î±Ï„ÏÎ¹ÎºÎ¬ ÎµÏÎ³Î±Î»ÎµÎ¯Î±, Ï‡ÎµÎ¹ÏÎ¿Ï…ÏÎ³Î¹ÎºÎ®, Ï€ÏÏŒÎ»Î·ÏˆÎ·, Î¹Î±Ï„ÏÎ¹ÎºÎ® Ï†ÏÎ¿Î½Ï„Î¯Î´Î±, ÏˆÏ…Ï‡Î¹ÎºÎ® Ï…Î³ÎµÎ¯Î±, Î¿Î´Î¿Î½Ï„Î¹Î±Ï„ÏÎ¹ÎºÎ®, Ï€Î±Î¹Î´Î¹Î±Ï„ÏÎ¹ÎºÎ®, Î³Ï…Î½Î±Î¹ÎºÎ¿Î»Î¿Î³Î¯Î±, ÎºÎ±ÏÎ´Î¹Î¿Î»Î¿Î³Î¯Î±, Î¿ÏÎ¸Î¿Ï€ÎµÎ´Î¹ÎºÎ®, Î´ÎµÏÎ¼Î±Ï„Î¿Î»Î¿Î³Î¯Î±",
                "keywords": ["Ï…Î³ÎµÎ¯Î±", "Î¹Î±Ï„ÏÎ¹ÎºÎ®", "Î½Î¿ÏƒÎ¿ÎºÎ¿Î¼ÎµÎ¯Î¿", "Î³Î¹Î±Ï„ÏÏŒÏ‚", "Ï†Î¬ÏÎ¼Î±ÎºÎ±", "ÎµÏ…ÎµÎ¾Î¯Î±", "Ï†Î±ÏÎ¼Î±ÎºÎµÎ¯Î¿", "ÎºÎ»Î¹Î½Î¹ÎºÎ®", "Î¸ÎµÏÎ±Ï€ÎµÎ¯Î±", "Î´Î¹Î¬Î³Î½Ï‰ÏƒÎ·", "Î±ÏƒÎ¸Î­Î½ÎµÎ¹Î±", "Î±ÏƒÎ¸ÎµÎ½Î®Ï‚", "Ï†ÏÎ¿Î½Ï„Î¯Î´Î±", "Î¹Î±Ï„ÏÎ¿Î¯", "Î½Î¿ÏƒÎ·Î»ÎµÏ…Ï„Î­Ï‚", "Ï†Î±ÏÎ¼Î±ÎºÎµÏ…Ï„Î¹ÎºÎ¬", "Ï‡ÎµÎ¹ÏÎ¿Ï…ÏÎ³Î¹ÎºÎ®", "Ï€ÏÏŒÎ»Î·ÏˆÎ·", "ÏˆÏ…Ï‡Î¹ÎºÎ®", "Î¿Î´Î¿Î½Ï„Î¹Î±Ï„ÏÎ¹ÎºÎ®", "Ï€Î±Î¹Î´Î¹Î±Ï„ÏÎ¹ÎºÎ®", "health", "medical", "doctor", "medicine"],
                "tlds": [".health", ".medical", ".care", ".clinic"],
                "domains": ["webmd", "mayoclinic", "healthline", "medscape", "nih", "who", "cdc", "medline", "pubmed"]
            },
            "ÎšÏ…Î²Î­ÏÎ½Î·ÏƒÎ· & Î”Î·Î¼ÏŒÏƒÎ¹ÎµÏ‚ Î¥Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚": {
                "english_name": "Government & Public Services",
                "description": "ÎºÏ…Î²Î­ÏÎ½Î·ÏƒÎ·, Î´Î·Î¼ÏŒÏƒÎ¹ÎµÏ‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚, Î´Î¹Î¿Î¯ÎºÎ·ÏƒÎ·, Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ®, ÎµÏ€Î¯ÏƒÎ·Î¼Î±, ÎºÏÎ¬Ï„Î¿Ï‚, Î¿Î¼Î¿ÏƒÏ€Î¿Î½Î´Î¹Î±ÎºÎ¬, Ï„Î¿Ï€Î¹ÎºÎ® ÎºÏ…Î²Î­ÏÎ½Î·ÏƒÎ·, Î´Î·Î¼ÏŒÏƒÎ¹Î¿Ï‚ Ï„Î¿Î¼Î­Î±Ï‚, Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ¬, ÎºÏ…Î²ÎµÏÎ½Î·Ï„Î¹ÎºÎ¬, ÎµÏ€Î¯ÏƒÎ·Î¼Î± Î¹Î´ÏÏÎ¼Î±Ï„Î±, Ï…Ï€Î¿Ï…ÏÎ³ÎµÎ¯Î±, Î´Î®Î¼Î¿Î¹, Ï€ÎµÏÎ¹Ï†Î­ÏÎµÎ¹ÎµÏ‚, ÎºÏ…Î²ÎµÏÎ½Î·Ï„Î¹ÎºÎ¿Î¯ Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼Î¿Î¯, Î´Î·Î¼ÏŒÏƒÎ¹Î± Î´Î¹Î¿Î¯ÎºÎ·ÏƒÎ·, Î½Î¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±, ÎºÎ±Î½Î¿Î½Î¹ÏƒÎ¼Î¿Î¯, Î´Î·Î¼ÏŒÏƒÎ¹ÎµÏ‚ Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ­Ï‚, ÎµÎºÎ»Î¿Î³Î­Ï‚, Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ¬ ÎºÏŒÎ¼Î¼Î±Ï„Î±, Î²Î¿Ï…Î»Î®, ÎºÎ¿Î¹Î½Î¿Î²Î¿ÏÎ»Î¹Î¿, Î´Î¹ÎºÎ±ÏƒÏ„Î®ÏÎ¹Î±, Î±ÏƒÏ„Ï…Î½Î¿Î¼Î¯Î±, ÏƒÏ„ÏÎ±Ï„ÏŒÏ‚, Î´Î·Î¼ÏŒÏƒÎ¹Î± Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±",
                "keywords": ["ÎºÏ…Î²Î­ÏÎ½Î·ÏƒÎ·", "Î´Î·Î¼ÏŒÏƒÎ¹ÎµÏ‚", "ÎµÏ€Î¯ÏƒÎ·Î¼Î±", "ÎºÏÎ¬Ï„Î¿Ï‚", "Î´Î¹Î¿Î¯ÎºÎ·ÏƒÎ·", "Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ®", "Ï€Î¿Î»Î¹Ï„Î¹ÎºÎ¬", "Ï…Ï€Î¿Ï…ÏÎ³ÎµÎ¯Î¿", "Ï„Î¼Î®Î¼Î±", "Î¿ÏÎ³Î±Î½Î¹ÏƒÎ¼ÏŒÏ‚", "Î³ÏÎ±Ï†ÎµÎ¯Î¿", "Î´Î®Î¼Î¿Ï‚", "Ï€ÎµÏÎ¹Ï†Î­ÏÎµÎ¹Î±", "Î½Î¿Î¼Î¿Î¸ÎµÏƒÎ¯Î±", "ÎºÎ±Î½Î¿Î½Î¹ÏƒÎ¼Î¿Î¯", "ÎµÎºÎ»Î¿Î³Î­Ï‚", "ÎºÏŒÎ¼Î¼Î±Ï„Î±", "Î²Î¿Ï…Î»Î®", "ÎºÎ¿Î¹Î½Î¿Î²Î¿ÏÎ»Î¹Î¿", "Î´Î¹ÎºÎ±ÏƒÏ„Î®ÏÎ¹Î±", "Î±ÏƒÏ„Ï…Î½Î¿Î¼Î¯Î±", "ÏƒÏ„ÏÎ±Ï„ÏŒÏ‚", "government", "public", "official"],
                "tlds": [".gov", ".mil", ".pol", ".gr"],
                "domains": ["usa", "uk", "europa", "un", "who", "nato", "government", "gov", "ypes", "minedu", "minhealth"]
            },
            "Î¤Î±Î¾Î¯Î´Î¹Î± & Î¤Î¿Ï…ÏÎ¹ÏƒÎ¼ÏŒÏ‚": {
                "english_name": "Travel & Tourism",
                "description": "Ï„Î±Î¾Î¯Î´Î¹Î±, Ï„Î¿Ï…ÏÎ¹ÏƒÎ¼ÏŒÏ‚, Î¾ÎµÎ½Î¿Î´Î¿Ï‡ÎµÎ¯Î±, Ï€Ï„Î®ÏƒÎµÎ¹Ï‚, Î´Î¹Î±ÎºÎ¿Ï€Î­Ï‚, Ï€ÏÎ¿Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯, ÎºÏÎ¬Ï„Î·ÏƒÎ·, Ï†Î¹Î»Î¿Î¾ÎµÎ½Î¯Î±, Î´Î¹Î±Î¼Î¿Î½Î®, ÎµÏƒÏ„Î¹Î±Ï„ÏŒÏÎ¹Î±, ÏƒÏ‡ÎµÎ´Î¹Î±ÏƒÎ¼ÏŒÏ‚ Ï„Î±Î¾Î¹Î´Î¹Î¿Ï, Ï€ÎµÏÎ¹Ï€Î­Ï„ÎµÎ¹Î±, ÎµÎ¾ÎµÏÎµÏÎ½Î·ÏƒÎ·, Ï„Î¿Ï…ÏÎ¹ÏƒÏ„Î¹ÎºÎ¬ Î±Î¾Î¹Î¿Î¸Î­Î±Ï„Î±, Ï„Î±Î¾Î¹Î´Î¹Ï‰Ï„Î¹ÎºÎ¿Î¯ Î¿Î´Î·Î³Î¿Î¯, Î±ÎµÏÎ¿Ï€Î¿ÏÎ¹ÎºÎ¬ ÎµÎ¹ÏƒÎ¹Ï„Î®ÏÎ¹Î±, ÎºÏÎ¿Ï…Î±Î¶Î¹Î­ÏÎµÏ‚, Ï„Î±Î¾Î¹Î´Î¹Ï‰Ï„Î¹ÎºÎ¬ Ï€Î±ÎºÎ­Ï„Î±, Ï„Î¿Ï…ÏÎ¹ÏƒÏ„Î¹ÎºÎ¿Î¯ Ï€ÏÎ¿Î¿ÏÎ¹ÏƒÎ¼Î¿Î¯, Î´Î¹Î±ÎºÎ¿Ï€Î­Ï‚, ÎµÎºÎ´ÏÎ¿Î¼Î­Ï‚, Ï„Î±Î¾Î¹Î´Î¹Ï‰Ï„Î¹ÎºÎ­Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚, Ï„Î±Î¾Î¹Î´Î¹Ï‰Ï„Î¹ÎºÎ¬ Î³ÏÎ±Ï†ÎµÎ¯Î±, Î¾ÎµÎ½ÏŽÎ½ÎµÏ‚, resort, camping, backpacking",
                "keywords": ["Ï„Î±Î¾Î¯Î´Î¹Î±", "Ï„Î¿Ï…ÏÎ¹ÏƒÎ¼ÏŒÏ‚", "Î¾ÎµÎ½Î¿Î´Î¿Ï‡ÎµÎ¯Î¿", "Ï€Ï„Î®ÏƒÎ·", "Î´Î¹Î±ÎºÎ¿Ï€Î­Ï‚", "Ï€ÏÎ¿Î¿ÏÎ¹ÏƒÎ¼ÏŒÏ‚", "ÎºÏÎ¬Ï„Î·ÏƒÎ·", "Ï†Î¹Î»Î¿Î¾ÎµÎ½Î¯Î±", "Î´Î¹Î±Î¼Î¿Î½Î®", "ÎµÏƒÏ„Î¹Î±Ï„ÏŒÏÎ¹Î¿", "Ï„Î±Î¾Î¯Î´Î¹", "Ï€ÎµÏÎ¹Ï€Î­Ï„ÎµÎ¹Î±", "ÎµÎ¾ÎµÏÎµÏÎ½Î·ÏƒÎ·", "Î¿Î´Î·Î³ÏŒÏ‚", "Î±Î¾Î¹Î¿Î¸Î­Î±Ï„Î±", "Î±ÎµÏÎ¿Ï€Î¿ÏÎ¹ÎºÎ¬", "ÎºÏÎ¿Ï…Î±Î¶Î¹Î­ÏÎµÏ‚", "Ï€Î±ÎºÎ­Ï„Î±", "ÎµÎºÎ´ÏÎ¿Î¼Î­Ï‚", "Î¾ÎµÎ½ÏŽÎ½ÎµÏ‚", "resort", "camping", "travel", "tourism", "hotel", "flight"],
                "tlds": [".travel", ".hotel", ".vacation", ".tour"],
                "domains": ["booking", "expedia", "airbnb", "tripadvisor", "hotels", "kayak", "priceline", "agoda", "trivago"]
            },
            "Î§ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬ & Î¤ÏÎ±Ï€ÎµÎ¶Î¹ÎºÎ¬": {
                "english_name": "Finance & Banking",
                "description": "Ï‡ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬, Ï„ÏÎ±Ï€ÎµÎ¶Î¹ÎºÎ¬, Ï‡ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ­Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚, ÎµÏ€ÎµÎ½Î´ÏÏƒÎµÎ¹Ï‚, Î±ÏƒÏ†Î¬Î»Î¹ÏƒÎ·, ÎºÏÏ…Ï€Ï„Î¿Î½Î¿Î¼Î¯ÏƒÎ¼Î±Ï„Î±, fintech, Ï‡ÏÎ®Î¼Î±Ï„Î±, Ï€Î»Î·ÏÏ‰Î¼Î®, Ï€Î¯ÏƒÏ„Ï‰ÏƒÎ·, Î´Î¬Î½ÎµÎ¹Î±, Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬, Î±Î³Î¿ÏÎ¬, ÏƒÏ…Î½Î±Î»Î»Î±Î³Î­Ï‚, Ï„ÏÎ¬Ï€ÎµÎ¶ÎµÏ‚, Ï‡ÏÎ·Î¼Î±Ï„Î¹ÏƒÏ„Î®ÏÎ¹Î¿, Î¼ÎµÏ„Î¿Ï‡Î­Ï‚, Î¿Î¼ÏŒÎ»Î¿Î³Î±, Ï‡ÏÎ·Î¼Î±Ï„Î¿Î´ÏŒÏ„Î·ÏƒÎ·, Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ® Î±Î½Î¬Î»Ï…ÏƒÎ·, Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ¬, Ï†Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ¬, Î±ÏƒÏ†Î±Î»Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚ ÎµÏ„Î±Î¹ÏÎµÎ¯ÎµÏ‚, Ï‡ÏÎ·Î¼Î±Ï„Î¿Ï€Î¹ÏƒÏ„Ï‰Ï„Î¹ÎºÎ¬ Î¹Î´ÏÏÎ¼Î±Ï„Î±, Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¿Î¯ ÏƒÏÎ¼Î²Î¿Ï…Î»Î¿Î¹, investment banking, private banking",
                "keywords": ["Ï‡ÏÎ·Î¼Î±Ï„Î¿Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬", "Ï„ÏÎ±Ï€ÎµÎ¶Î¹ÎºÎ¬", "ÎµÏ€ÎµÎ½Î´ÏÏƒÎµÎ¹Ï‚", "Î±ÏƒÏ†Î¬Î»Î¹ÏƒÎ·", "Ï‡ÏÎ®Î¼Î±Ï„Î±", "Ï€Î»Î·ÏÏ‰Î¼Î®", "Ï€Î¯ÏƒÏ„Ï‰ÏƒÎ·", "Î´Î¬Î½ÎµÎ¹Î±", "Î¿Î¹ÎºÎ¿Î½Î¿Î¼Î¹ÎºÎ¬", "Î±Î³Î¿ÏÎ¬", "ÏƒÏ…Î½Î±Î»Î»Î±Î³Î­Ï‚", "Ï„ÏÎ¬Ï€ÎµÎ¶ÎµÏ‚", "Ï‡ÏÎ·Î¼Î±Ï„Î¹ÏƒÏ„Î®ÏÎ¹Î¿", "Î¼ÎµÏ„Î¿Ï‡Î­Ï‚", "Î¿Î¼ÏŒÎ»Î¿Î³Î±", "Ï‡ÏÎ·Î¼Î±Ï„Î¿Î´ÏŒÏ„Î·ÏƒÎ·", "Î»Î¿Î³Î¹ÏƒÏ„Î¹ÎºÎ¬", "Ï†Î¿ÏÎ¿Î»Î¿Î³Î¹ÎºÎ¬", "Î±ÏƒÏ†Î±Î»Î¹ÏƒÏ„Î¹ÎºÎ­Ï‚", "Ï‡ÏÎ·Î¼Î±Ï„Î¿Ï€Î¹ÏƒÏ„Ï‰Ï„Î¹ÎºÎ¬", "ÏƒÏÎ¼Î²Î¿Ï…Î»Î¿Î¹", "finance", "banking", "investment", "insurance"],
                "tlds": [".bank", ".finance", ".money", ".insurance"],
                "domains": ["paypal", "visa", "mastercard", "american express", "chase", "wells fargo", "goldman sachs", "morgan stanley", "bloomberg", "reuters", "alphabank", "eurobank", "nbg", "piraeusbank"]
            },
            "Î‘Î¸Î»Î·Ï„Î¹ÏƒÎ¼ÏŒÏ‚ & Î‘Î½Î±ÏˆÏ…Ï‡Î®": {
                "english_name": "Sports & Recreation",
                "description": "Î±Î¸Î»Î·Ï„Î¹ÏƒÎ¼ÏŒÏ‚, Î±Î¸Î»Î·Ï„Î¹ÎºÎ¬, Î±Î½Î±ÏˆÏ…Ï‡Î®, Ï†Ï…ÏƒÎ¹ÎºÎ® ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·, Ï€Î±Î¹Ï‡Î½Î¯Î´Î¹Î±, Î´Î¹Î±Î³Ï‰Î½Î¹ÏƒÎ¼ÏŒÏ‚, Î¿Î¼Î¬Î´ÎµÏ‚, Ï€ÏÏ‰Ï„Î±Î¸Î»Î®Î¼Î±Ï„Î±, Î±Î¸Î»Î·Ï„Î­Ï‚, Î±Î¸Î»Î·Ï„Î¹ÎºÎ¬ Î³ÎµÎ³Î¿Î½ÏŒÏ„Î±, Ï…Ï€Î±Î¯Î¸ÏÎ¹ÎµÏ‚ Î´ÏÎ±ÏƒÏ„Î·ÏÎ¹ÏŒÏ„Î·Ï„ÎµÏ‚, Î¬ÏƒÎºÎ·ÏƒÎ·, Ï†Ï…ÏƒÎ¹ÎºÎ® Î´ÏÎ±ÏƒÏ„Î·ÏÎ¹ÏŒÏ„Î·Ï„Î±, Ï€ÏÎ¿Ï€ÏŒÎ½Î·ÏƒÎ·, Î³Ï…Î¼Î½Î±ÏƒÏ„Î®ÏÎ¹Î±, Î±Î¸Î»Î·Ï„Î¹ÎºÎ¬ ÎºÎ­Î½Ï„ÏÎ±, Î±Î¸Î»Î·Ï„Î¹ÎºÎ­Ï‚ ÎµÎ³ÎºÎ±Ï„Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚, Ï€Î¿Î´ÏŒÏƒÏ†Î±Î¹ÏÎ¿, Î¼Ï€Î¬ÏƒÎºÎµÏ„, Ï„Î­Î½Î¹Ï‚, ÎºÎ¿Î»ÏÎ¼Î²Î·ÏƒÎ·, Ï„ÏÎ­Î¾Î¹Î¼Î¿, Ï€Î¿Î´Î·Î»Î±ÏƒÎ¯Î±, Î³Ï…Î¼Î½Î±ÏƒÏ„Î¹ÎºÎ®, Î²ÏŒÎ»ÎµÏŠ, Ï‡Î¬Î½Ï„Î¼Ï€Î¿Î», Î±Î¸Î»Î·Ï„Î¹ÎºÎ­Ï‚ ÎµÎ¹Î´Î®ÏƒÎµÎ¹Ï‚, Î±Î¸Î»Î·Ï„Î¹ÎºÎ¬ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±",
                "keywords": ["Î±Î¸Î»Î·Ï„Î¹ÏƒÎ¼ÏŒÏ‚", "Î±Î¸Î»Î·Ï„Î¹ÎºÎ¬", "Î±Î½Î±ÏˆÏ…Ï‡Î®", "Ï†Ï…ÏƒÎ¹ÎºÎ® ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ·", "Ï€Î±Î¹Ï‡Î½Î¯Î´Î¹Î±", "Î´Î¹Î±Î³Ï‰Î½Î¹ÏƒÎ¼ÏŒÏ‚", "Î¿Î¼Î¬Î´ÎµÏ‚", "Ï€ÏÏ‰Ï„Î±Î¸Î»Î®Î¼Î±Ï„Î±", "Î±Î¸Î»Î·Ï„Î­Ï‚", "Î¬ÏƒÎºÎ·ÏƒÎ·", "Ï†Ï…ÏƒÎ¹ÎºÎ® Î´ÏÎ±ÏƒÏ„Î·ÏÎ¹ÏŒÏ„Î·Ï„Î±", "Ï€ÏÎ¿Ï€ÏŒÎ½Î·ÏƒÎ·", "Î³Ï…Î¼Î½Î±ÏƒÏ„Î®ÏÎ¹Î±", "Ï€Î¿Î´ÏŒÏƒÏ†Î±Î¹ÏÎ¿", "Î¼Ï€Î¬ÏƒÎºÎµÏ„", "Ï„Î­Î½Î¹Ï‚", "ÎºÎ¿Î»ÏÎ¼Î²Î·ÏƒÎ·", "Ï„ÏÎ­Î¾Î¹Î¼Î¿", "Ï€Î¿Î´Î·Î»Î±ÏƒÎ¯Î±", "Î³Ï…Î¼Î½Î±ÏƒÏ„Î¹ÎºÎ®", "Î²ÏŒÎ»ÎµÏŠ", "Ï‡Î¬Î½Ï„Î¼Ï€Î¿Î»", "sport", "sports", "fitness", "athletic"],
                "tlds": [".sport", ".fitness", ".team"],
                "domains": ["espn", "nba", "nfl", "fifa", "olympics", "sport", "athletic", "fitness", "gazzetta", "sport24", "contra", "novasports"]
            },
            "Î†Î»Î»Î¿": {
                "english_name": "Other",
                "description": "Î´Î¹Î¬Ï†Î¿ÏÎ±, Î³ÎµÎ½Î¹ÎºÎ¬, Î¼Î¹ÎºÏ„ÏŒ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿, Î¼Î· ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î· ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±, Î´Î¹Î¬Ï†Î¿ÏÎ± Î¸Î­Î¼Î±Ï„Î±, Ï€Î¿Î¹ÎºÎ¯Î»Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿, Î³ÎµÎ½Î¹ÎºÎ­Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚, Î±ÏƒÎ±Ï†Î®Ï‚ ÏƒÎºÎ¿Ï€ÏŒÏ‚, Î±Î½Î¬Î¼ÎµÎ¹ÎºÏ„Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿, Î¼Î· Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Î¼Î­Î½Î¿, Î³ÎµÎ½Î¹ÎºÎ¿Ï ÎµÎ½Î´Î¹Î±Ï†Î­ÏÎ¿Î½Ï„Î¿Ï‚, Ï€Î¿Î»Î»Î±Ï€Î»Î­Ï‚ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯ÎµÏ‚, Î±ÏƒÎ±Ï†Î­Ï‚ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿, Î¼Î· ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿ Î¸Î­Î¼Î±, Ï€Î¿Î¹ÎºÎ¹Î»Î¯Î± Î¸ÎµÎ¼Î¬Ï„Ï‰Î½, Î³ÎµÎ½Î¹ÎºÎ­Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚",
                "keywords": ["Î³ÎµÎ½Î¹ÎºÎ¬", "Î´Î¹Î¬Ï†Î¿ÏÎ±", "Î¼Î¹ÎºÏ„ÏŒ", "Î¬Î»Î»Î¿", "Î¼Î· ÎºÎ±Î¸Î¿ÏÎ¹ÏƒÎ¼Î­Î½Î¿", "Ï€Î¿Î¹ÎºÎ¯Î»Î¿", "Î´Î¹Î±Ï†Î¿ÏÎµÏ„Î¹ÎºÏŒ", "Î±Î½Î¬Î¼ÎµÎ¹ÎºÏ„Î¿", "Ï„Î±Î¾Î¹Î½Î¿Î¼Î·Î¼Î­Î½Î¿", "Î³ÎµÎ½Î¹ÎºÎ¿Ï", "ÎµÎ½Î´Î¹Î±Ï†Î­ÏÎ¿Î½Ï„Î¿Ï‚", "Ï€Î¿Î»Î»Î±Ï€Î»Î­Ï‚", "Î±ÏƒÎ±Ï†Î­Ï‚", "ÏƒÏ…Î³ÎºÎµÎºÏÎ¹Î¼Î­Î½Î¿", "Ï€Î¿Î¹ÎºÎ¹Î»Î¯Î±", "Î³ÎµÎ½Î¹ÎºÎ­Ï‚", "Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚", "general", "misc", "various", "mixed", "other"],
                "tlds": [],
                "domains": []
            }
        }
        
        # Create Greek to English mapping
        self.greek_to_english = {greek_name: info['english_name'] for greek_name, info in self.categories.items()}
        
        print(f"ðŸš€ Enhanced Domain Classifier initialized on {self.device}")
    
    def _convert_to_english(self, greek_category):
        """Convert Greek category name to English for output"""
        # Handle asterisk prefix for conflicts
        if greek_category.startswith('*'):
            greek_name = greek_category[1:]
            english_name = self.greek_to_english.get(greek_name, greek_name)
            return f"*{english_name}"
        else:
            return self.greek_to_english.get(greek_category, greek_category)
    
    def setup_models(self):
        """Initialize ensemble of embedding models"""
        print("ðŸ“¥ Loading embedding models...")
        
        # Original model configurations - fixed model names
        model_configs = [
            ("e5-large", "intfloat/multilingual-e5-large"),
            ("mpnet", "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"),
            ("minilm", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"),
            ("labse", "sentence-transformers/LaBSE"),
            ("jina", "jinaai/jina-embeddings-v3"),
            ("mdeberta", "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7"),
        ]
        
        # Use /mnt/data for cache to avoid disk space issues
        import os
        cache_dir = "/mnt/data/jimmy/huggingface_cache"
        os.makedirs(cache_dir, exist_ok=True)
        
        # Set environment variables to force HuggingFace to use our cache directory
        os.environ['HUGGINGFACE_HUB_CACHE'] = cache_dir
        os.environ['TRANSFORMERS_CACHE'] = cache_dir
        os.environ['HF_HOME'] = cache_dir
        
        successful_models = []
        failed_models = []
        
        for name, model_name in model_configs:
            try:
                print(f"   Loading {name}...")
                
                # Try loading with different strategies
                model = None
                
                if name == "jina":
                    # Try different Jina models
                    jina_models = [
                        "jinaai/jina-embeddings-v3",
                        "jinaai/jina-embeddings-v2-base-en", 
                        "jinaai/jina-embeddings-v2-small-en"
                    ]
                    for jina_model in jina_models:
                        try:
                            print(f"     Trying {jina_model}...")
                            model = SentenceTransformer(jina_model, cache_folder=cache_dir, trust_remote_code=True)
                            break
                        except Exception as e:
                            print(f"     Failed {jina_model}: {str(e)[:100]}...")
                            continue
                            
                elif name == "mdeberta":
                    # Try different mDeBERTa models
                    mdeberta_models = [
                        "MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7",
                        "microsoft/mdeberta-v3-base",
                        "microsoft/deberta-v3-base"
                    ]
                    for mdeberta_model in mdeberta_models:
                        try:
                            print(f"     Trying {mdeberta_model}...")
                            model = SentenceTransformer(mdeberta_model, cache_folder=cache_dir, trust_remote_code=True)
                            break
                        except Exception as e:
                            print(f"     Failed {mdeberta_model}: {str(e)[:100]}...")
                            continue
                            
                elif name == "labse":
                    # Try different LaBSE models
                    labse_models = [
                        "sentence-transformers/LaBSE",
                        "cointegrated/LaBSE-en-ru"
                    ]
                    for labse_model in labse_models:
                        try:
                            print(f"     Trying {labse_model}...")
                            model = SentenceTransformer(labse_model, cache_folder=cache_dir)
                            break
                        except Exception as e:
                            print(f"     Failed {labse_model}: {str(e)[:100]}...")
                            continue
                else:
                    # Standard loading for other models
                    model = SentenceTransformer(model_name, cache_folder=cache_dir)
                
                if model is not None:
                    model = model.to(self.device)
                    self.models[name] = model
                    successful_models.append(name)
                    print(f"   âœ… {name} loaded successfully")
                else:
                    failed_models.append(name)
                    print(f"   âŒ All attempts failed for {name}")
                    
            except Exception as e:
                failed_models.append(name)
                print(f"   âŒ Failed to load {name}: {str(e)[:100]}...")
                continue
        
        print(f"\nðŸ“Š Model Loading Summary:")
        print(f"   âœ… Successfully loaded: {len(successful_models)} models: {', '.join(successful_models)}")
        if failed_models:
            print(f"   âŒ Failed to load: {len(failed_models)} models: {', '.join(failed_models)}")
        
        if not self.models:
            raise RuntimeError("No embedding models could be loaded!")
        
        print(f"âœ… Loaded {len(self.models)} embedding models")
        
        # Pre-compute category embeddings for each model
        self._compute_category_embeddings()
    
    def _compute_category_embeddings(self):
        """Pre-compute embeddings for all categories using all models"""
        print("ðŸ§® Computing category embeddings...")
        
        for model_name, model in self.models.items():
            print(f"   Computing embeddings with {model_name}...")
            category_texts = []
            category_names = []
            
            for category, info in self.categories.items():
                # Create rich text representation
                text = f"Category: {category}. Description: {info['description']}. Keywords: {' '.join(info['keywords'][:20])}"
                category_texts.append(text)
                category_names.append(category)
            
            embeddings = model.encode(category_texts, convert_to_tensor=True, device=self.device, show_progress_bar=False)
            self.category_embeddings[model_name] = {
                'embeddings': embeddings,
                'names': category_names
            }
        
        print("âœ… Category embeddings computed")
    

    
    def extract_domain_features(self, domain_name):
        """Extract domain-specific features"""
        try:
            domain = domain_name.lower().strip()
            
            # Remove www prefix if present
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # Extract TLD
            tld = domain.split('.')[-1] if '.' in domain else ''
            
            # Extract domain name without TLD and convert dots to spaces
            if '.' in domain:
                domain_parts = domain.split('.')
                base_domain_name = '.'.join(domain_parts[:-1])  # Remove TLD
                processed_domain_name = base_domain_name.replace('.', ' ')  # Replace dots with spaces
            else:
                processed_domain_name = domain
            
            return {
                'domain': domain,
                'domain_name': processed_domain_name,
                'tld': tld
            }
        except:
            return {
                'domain': domain_name,
                'domain_name': domain_name.replace('.', ' '),
                'tld': ''
            }
    
    def preprocess_text(self, row):
        """Enhanced text preprocessing"""
        parts = []
        
        # Extract domain features
        domain_features = self.extract_domain_features(row.get('domain', ''))
        
        # Check if site has any metadata (title, description, keywords)
        has_metadata = False
        metadata_fields = ['title', 'meta_description', 'keywords']
        
        for field in metadata_fields:
            if field in row and pd.notna(row[field]):
                text = str(row[field]).strip()
                if text and text.lower() not in ['none', 'null', 'nan', '']:
                    has_metadata = True
                    parts.append(text)
        
        # Add domain name as separate feature
        if domain_features['domain_name']:
            parts.append(domain_features['domain_name'])
        
        combined_text = ' '.join(parts) if parts else ''
        
        # Store metadata status in domain_features for later use
        domain_features['has_metadata'] = has_metadata
        
        return combined_text, domain_features
    
    def classify_with_ensemble(self, text, domain_features, retry_count=0):
        """Classify using ensemble of models with conflict resolution"""
        if not text:
            return "Î†Î»Î»Î¿", 0.1
        
        # Automatically assign "Other" if site has no metadata
        if not domain_features.get('has_metadata', True):
            return "Î†Î»Î»Î¿", 0.1
        
        # Truncate text if too long
        text = text[:3000] if len(text) > 3000 else text
        
        predictions = []
        confidences = []
        
        # Get predictions from each model
        for model_name, model in self.models.items():
            try:
                # Encode text
                text_embedding = model.encode([text], convert_to_tensor=True, device=self.device, show_progress_bar=False)
                
                # Calculate similarities
                category_data = self.category_embeddings[model_name]
                similarities = cosine_similarity(
                    text_embedding.cpu().numpy(),
                    category_data['embeddings'].cpu().numpy()
                )[0]
                
                # Get best prediction (exclude "Other" from initial classification)
                filtered_categories = [cat for cat in category_data['names'] if cat != "Î†Î»Î»Î¿"]
                filtered_similarities = []
                filtered_names = []
                
                for i, cat_name in enumerate(category_data['names']):
                    if cat_name != "Î†Î»Î»Î¿":
                        filtered_similarities.append(similarities[i])
                        filtered_names.append(cat_name)
                
                if filtered_similarities:
                    best_idx = np.argmax(filtered_similarities)
                    best_category = filtered_names[best_idx]
                    confidence = float(filtered_similarities[best_idx])
                else:
                    # Fallback if no categories available
                    best_idx = np.argmax(similarities)
                    best_category = category_data['names'][best_idx]
                    confidence = float(similarities[best_idx])
                
                predictions.append(best_category)
                confidences.append(confidence)
                
            except Exception as e:
                logger.warning(f"Error with model {model_name}: {e}")
                continue
        
        if not predictions:
            return "Î†Î»Î»Î¿", 0.1
        
        # Check for conflicts (ties in predictions)
        prediction_counts = Counter(predictions)
        max_count = max(prediction_counts.values())
        tied_categories = [cat for cat, count in prediction_counts.items() if count == max_count]
        
        # If there's a tie and we haven't exceeded retry limit
        if len(tied_categories) > 1 and retry_count < 3:
            # Add some randomness to break ties by slightly modifying text
            random_words = random.sample(['domain', 'website', 'site', 'content', 'page', 'portal'], 2)
            modified_text = text + f" {' '.join(random_words)}"
            return self.classify_with_ensemble(modified_text, domain_features, retry_count + 1)
        
        # Apply domain-specific rules
        enhanced_prediction = self._apply_domain_rules(predictions, confidences, text, domain_features)
        
        # Check if results are too vague (low confidence across all models)
        avg_confidence = np.mean(confidences)
        if avg_confidence < 0.3 and len(set(predictions)) >= len(predictions) * 0.7:
            # Too many different predictions with low confidence
            return "Î†Î»Î»Î¿", avg_confidence
        
        # If we had ties and exceeded retry limit, mark with asterisk
        if len(tied_categories) > 1 and retry_count >= 3:
            category, confidence = enhanced_prediction
            return f"*{category}", confidence
        
        return enhanced_prediction
    
    def _apply_domain_rules(self, predictions, confidences, text, domain_features):
        """Apply domain-specific rules to enhance predictions"""
        text_lower = text.lower()
        domain_name = domain_features['domain_name'].lower()
        tld = domain_features['tld'].lower()
        
        # Rule-based enhancements (exclude "Other" from rule matching)
        rule_boosts = defaultdict(float)
        
        # Check for domain name patterns
        for category, info in self.categories.items():
            if category == "Î†Î»Î»Î¿":
                continue  # Skip "Other" category in rule matching
                
            # Check domain names
            for domain_pattern in info['domains']:
                if domain_pattern in domain_name:
                    rule_boosts[category] += 0.3
            
            # Check TLDs
            for tld_pattern in info['tlds']:
                if tld_pattern.lstrip('.') == tld:
                    rule_boosts[category] += 0.2
            
            # Check keywords in text
            keyword_matches = sum(1 for keyword in info['keywords'] if keyword in text_lower)
            if keyword_matches > 0:
                rule_boosts[category] += min(0.2, keyword_matches * 0.05)
        
        # Combine ensemble predictions with rules
        category_scores = defaultdict(float)
        
        # Add ensemble scores (exclude "Other" predictions)
        for pred, conf in zip(predictions, confidences):
            if pred != "Î†Î»Î»Î¿":
                category_scores[pred] += conf
        
        # Apply rule boosts
        for category, boost in rule_boosts.items():
            if category != "Î†Î»Î»Î¿":
                category_scores[category] += boost
        
        # Get final prediction
        if not category_scores:
            return "Î†Î»Î»Î¿", 0.1
        
        best_category_name = max(category_scores.items(), key=lambda x: x[1])[0]
        
        # Calculate confidence as average of only the scores that predicted this category
        matching_confidences = []
        for pred, conf in zip(predictions, confidences):
            if pred == best_category_name:
                matching_confidences.append(conf)
        
        # If no direct matches (only rule-based), use a base confidence
        if not matching_confidences:
            final_confidence = 0.5
        else:
            final_confidence = sum(matching_confidences) / len(matching_confidences)
        
        return best_category_name, min(1.0, final_confidence)
    

    
    def process_data(self, df, batch_size=500, output_file="classified_domains.parquet"):
        """Process data with enhanced classification"""
        print(f"ðŸ”„ Processing {len(df)} domains in batches of {batch_size}...")
        
        # Check for existing results to resume from
        start_batch = 0
        existing_results = []
        
        if os.path.exists(output_file):
            try:
                existing_df = pd.read_parquet(output_file)
                existing_count = len(existing_df)
                start_batch = existing_count // batch_size
                existing_results = existing_df.to_dict('records')
                
                print(f"ðŸ“‹ Found existing results: {existing_count} domains already processed")
                print(f"ðŸ”„ Resuming from batch {start_batch + 1} (starting at domain {existing_count + 1})")
                
                # Skip already processed domains
                df = df.iloc[existing_count:].reset_index(drop=True)
                
            except Exception as e:
                print(f"âš ï¸  Could not read existing results: {e}")
                print(f"ðŸ”„ Starting fresh...")
                start_batch = 0
                existing_results = []
        
        if len(df) == 0:
            print("âœ… All domains already processed!")
            return pd.read_parquet(output_file) if os.path.exists(output_file) else pd.DataFrame()
        
        results = existing_results.copy()
        all_texts = []
        
        # Calculate total number of batches for remaining data
        total_batches = (len(df) + batch_size - 1) // batch_size
        total_batches_overall = start_batch + total_batches
        
        # Process in batches with overall progress bar
        batch_progress = tqdm(range(total_batches), desc="Processing batches", unit="batch")
        
        for batch_num in batch_progress:
            start_idx = batch_num * batch_size
            end_idx = min(start_idx + batch_size, len(df))
            batch = df.iloc[start_idx:end_idx]
            
            batch_results = []
            batch_texts = []
            
            # Calculate actual batch number (including already processed)
            actual_batch_num = start_batch + batch_num + 1
            actual_domain_start = len(existing_results) + start_idx + 1
            actual_domain_end = len(existing_results) + end_idx
            
            # Update batch progress description
            batch_progress.set_description(f"Batch {actual_batch_num}/{total_batches_overall} (domains {actual_domain_start}-{actual_domain_end})")
            
            # Progress bar for domains within this batch
            domain_desc = f"  â””â”€ Classifying domains"
            
            for idx, row in tqdm(batch.iterrows(), total=len(batch), desc=domain_desc, leave=False, position=1):
                # Preprocess
                text, domain_features = self.preprocess_text(row)
                
                # Classify
                category, confidence = self.classify_with_ensemble(text, domain_features)
                
                # Convert Greek category to English for output
                english_category = self._convert_to_english(category)
                
                # Store result - preserve all original columns and add new ones
                result = row.to_dict()  # Keep all original columns
                result['category'] = english_category  # Add category
                result['confidence'] = confidence      # Add confidence
                
                batch_results.append(result)
                if text:
                    batch_texts.append(text)
                    all_texts.append(text)
            
            results.extend(batch_results)
            
            # Save progress after each batch (incremental save)
            try:
                results_df = pd.DataFrame(results)
                results_df.to_parquet(output_file, index=False)
            except Exception as e:
                tqdm.write(f"âš ï¸  Warning: Could not save progress: {e}")
            
            # Update batch progress with completion info
            batch_progress.set_postfix({
                'domains': f"{len(batch_results)}",
                'total_processed': f"{len(results)}"
            })
            
            # Memory cleanup every 5 batches (2500 domains)
            if actual_batch_num % 5 == 0:
                gc.collect()
                tqdm.write(f"ðŸ§¹ Memory cleanup after {actual_batch_num} batches")
        
        batch_progress.close()
        
        # Create results DataFrame
        results_df = pd.DataFrame(results)
        

        
        # Save results
        results_df.to_parquet(output_file, index=False)
        print(f"âœ… Results saved to {output_file}")
        
        # Print summary statistics
        self._print_summary(results_df)
        
        return results_df
    
    def _print_summary(self, results_df):
        """Print classification summary"""
        print("\nðŸ“ˆ Classification Summary:")
        print("=" * 50)
        
        category_counts = results_df['category'].value_counts()
        for category, count in category_counts.items():
            percentage = (count / len(results_df)) * 100
            avg_confidence = results_df[results_df['category'] == category]['confidence'].mean()
            conflict_marker = " (CONFLICT RESOLVED)" if category.startswith('*') else ""
            print(f"{category:30s}: {count:6d} ({percentage:5.1f}%) - Avg Confidence: {avg_confidence:.3f}{conflict_marker}")
        
        print(f"\nTotal domains classified: {len(results_df):,}")
        print(f"Average confidence: {results_df['confidence'].mean():.3f}")
        print(f"High confidence (>0.7): {len(results_df[results_df['confidence'] > 0.7]):,} ({len(results_df[results_df['confidence'] > 0.7])/len(results_df)*100:.1f}%)")
        
        # Additional statistics
        conflicts = len(results_df[results_df['category'].str.startswith('*')])
        vague = len(results_df[results_df['category'] == 'Other'])
        
        print(f"\nðŸ”„ Conflict Resolution Statistics:")
        print(f"Domains with conflicts (marked with *): {conflicts:,} ({conflicts/len(results_df)*100:.1f}%)")
        print(f"Vague classifications (Other): {vague:,} ({vague/len(results_df)*100:.1f}%)")
        print(f"Successfully classified: {len(results_df) - vague:,} ({(len(results_df) - vague)/len(results_df)*100:.1f}%)")


def main():
    """Main function"""
    if len(sys.argv) != 2:
        print("Usage: python enhanced_domain_classifier.py <oscar_metadata.parquet>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    if not os.path.exists(input_file):
        print(f"Error: File {input_file} not found!")
        sys.exit(1)
    
    print("ðŸš€ Enhanced Domain Classification Starting...")
    print("=" * 60)
    
    # Load data
    print(f"ðŸ“– Loading data from {input_file}...")
    df = pd.read_parquet(input_file)
    print(f"   Loaded {len(df):,} domains")
    
    # Initialize classifier
    classifier = EnhancedDomainClassifier(use_gpu=True)
    
    # Setup models
    classifier.setup_models()
    
    # Process data
    output_file = input_file.replace('.parquet', '_enhanced_classified.parquet')
    results_df = classifier.process_data(df, output_file=output_file)
    
    print("\nðŸŽ‰ Enhanced classification completed!")
    print(f"ðŸ“Š Results saved to: {output_file}")


if __name__ == "__main__":
    main() 